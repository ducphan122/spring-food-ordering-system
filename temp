Hi everyone

In this lecture I will continue changing

the payment Kafka message listener to consume messages

that is published by Debezium connector.

Now I will mention about my load test scenario

that I will implement in the next lecture using JMeter.

If we consider this food ordering system application

with a front end, a single customer

usually creates a single order at a time.

I wouldn't expect a customer

creating more than one order simultaneously.

However, consider a scenario that we give the customer ID

to a company and they use it in their system internally

for all their workers.

So all workers of the company will use the same customer ID

and can create orders simultaneously.

Actually, I will create a load test

using a single customer ID

and create up to 3000 order requests

that is consumed by three different order service instances.

I will use three instances of order service,

payment service and restaurant service

to take advantage of three partitions

that are used in the Kafka topics.

Remember that the partition number

is the natural way of creating a parallelism

in Kafka processing.

So with three partitions

I can go up to three consumers, that will run concurrently.

Now in this scenario where I use a single customer ID

for a lot of requests coming at the same time,

there is a high chance of getting concurrent update issues.

So if I have a list of inputs

and process the first N number of items,

but get an error in the N plus one item,

I will need to reprocess the first N items,

which will create a unique constraint exception

which will be eliminated here in this catch.

However, since I expect this occur too much,

that will decrease my overall performance.

So instead of using a list,

I will prefer to use a single item in this listener method.

So why do I expect that much issue here?

Let's look at that before starting doing changes.

If you look at the persistPayment implementation

in the PaymentRequestHelper,

we have a check for credit entry and credit history.

Imagine multiple requests arrived with same customer ID

after fetching a credit entry in a thread

before fetching the history.

Another thread might already the updated history.

So in that case I will get inconsistent credit entry

and history which will cause the business validation

to fail.

As you see, I have a business validation

for credit entry and history to be consistent.

To solve this issue I have two options,

applying pessimistic locking

or applying optimistic locking.

I will apply both versions in different branches

and compare the performance test results.

Usually in case of too many collisions,

which requires to roll back previous operations,

optimistic locking will be slower,

so better to prefer pessimistic locking.

However, pessimistic locking will actually serialize

all the incoming requests.

So I will not have any concurrent processing

at this point of the data flow in payment service.

Pessimistic locking is easier to implement,

so let's start with that.

If I use pessimistic lock on the getCreditEntry method

it will prevent all other concurrent threads to wait

in their getCreditEntry calls

until the lock is released.

And the log is released only when the current transaction

is committed or rolled back.

Remember that the transaction is committed

after returning from this method.

So the next thread that gets the lock,

will see the latest customer credit entry.

Since the getCreditHistory method is called

after getCreditEntry, there is no chance

of any other thread changing the history data

after getting the credit entry,

because now other threads will be blocked

until the lock is released

by the current running thread.







All right, let's start changing the code.

First of all, please be sure you change the publisher code

as in the restaurant message listener.

As I mentioned, the publishing is done through Debezium now,

because there is no chance to get two same messages

on Kafka, only Debezium can retry if there is an error,

but no two messages can come from Kafka,

as in the pulling the outbox table solution.

So here we don't publish a message,

but just check if a message is already processed

and return true in that case.

Then I will go to findByCustomerId method

in credit entry JPA repository.

And just add Lock annotation,

with LockModeType.PESSIMISTIC_WRITE,

this will actually get a select for update lock

if you are familiar with SQL language.

So it'll put a lock on this customer row

and do not allow another select for update

until the transaction is completed,

which will serialize the multiple transactions

and only allow one at a time.

Now because of this pessimistic lock,

a thread might wait for a time

and may also get a timeout exception.

Or when we implement optimistic locking

it can get an optimistic lock exception multiple times,

which may cause the offset commit timeout to be expired,

so that a single message can arrive multiple times

from Kafka into this listener.

So as I mentioned,

I will reduce this list to a single Avro model

to prevent reprocessing the previous items.

For this I will create a new interface,

KafkaSingleItemConsumer,

and use a single item, T, for the first parameter.

And also use single item for key, partition, and offset.

Then I will implement this

in the PaymentRequestKafkaListener,

with the Envelope class from

debezium.order.payment_outbox package.

Let's update the receive method to use a single envelope.

Then I will also update the key, partition and offset

to get them for a single item instead of a list.

Remember that I was using

a batch listener configuration property

and configured BatchListener in the Kafka consumer config

as you see here.

So for payment container application YAML file

I will set this value as false.

So batch listener is false here.

Now here in this listener I don't need a for loop.

I will check the message directly

that the before object is null

and the operation is a CREATE operation.

To use DebeziumOp enum class,

I will add a dependency

to common messaging here in this module.

I will log the incoming message

after this check, including the key, partition and offset.

Let's get this message inside the if block here.

I will fix this typo in offset parameter.

Then I will get the after object value

and then use OrderPaymentsEventPayload class

from the common domain module

under the event payload package,

as I did for other services.

I will inject KafkaMessageHelper

using constructor injection here

and use getOrderEventPayload methods

with paymentRequestAyroModel payload,

and OrderPaymentEventPayload class parameters

and get orderPaymentEventPayload object.

I will update the if conditions

to use the payment order status string value

with equals methods

using the orderPaymentEventPayload object

and change paymentRequestAyroModel

to paymentRequest methods in the data mapper class

and convert orderPaymentEventPayload

and paymentRequestAyroModel to paymentRequest object.

Then I will change the method calls

and use orderPaymentEventPayload

and paymentRequestAyroModel parameters here.

I will then change the log statements

saying use orderPaymentEventPayload to get the order ID.

Before I continue, here I compare the name

of the DebeziumOp.CREATE definition.

However, I should actually compare the value

of the CREATE enum, which is c.

So I will go to DebeziumOp enum class

and create getValue method to return the value.

And here in the if statement

I will compare the message operation

with the value of the CREATE enum, which is c.

Actually, I have the same issue in other Kafka listeners.

Let's go and fix them

in the RestaurantApprovalRequestKafkaListener

in the restaurant messaging module,

I will use getValue methods

instead of name in if statements.

I will also update the log statement here

to use getValue method.

Then I will go to order messaging module

and update the Kafka listener classes.

First in paymentResponse Kafka listener,

I will update if condition to use getValue method

of DebeziumOp and update the log statement.

And then in the RestaurantApprovalRequestKafkaListener class

I will update the if statements,

again to use getValue methods.

Finally, I will update this log statement as well.

I will then go to application YAML file

in payment container module and change the payment request

and payment response topic names.

I will use debezium.order.payment_outbox

for the request topic name

and debezium.payment.order_outbox

for the response topic name.

Then I will delete the scheduler config values

as we don't use them anymore.

You can double check the topic names

by looking at the package structure

and Avro schema files under the kafka-model module.

All right now we can run the services

and try the change data capture with Debezium in action.

First, I will use my startup shell script

to start Zookeeper and Kafka cluster,

create the Kafka topics and create Debezium connectors.

Please be sure you give execute permission

to the script files using chmod +x command here.

Now I can run the startup script.

I will check if all containers are started correctly

in a new terminal window using the docker ps command.

Then I will run my services: customer service,

order service,

payment service and restaurant service.

Great, let's first create a customer

using customer service endpoint in Postman

using 8184 port.

Then I will send a create order request using

order service post endpoint.

with 8181 port.

Here you see that the price is 50 for this product.

Okay, now let's check the order service get endpoint.

As you see, the order is already approved.

Let's check the logs of the order service,

payment service and restaurant service.

As you can see, we have the message

that is published using Debezium connector.

It has after object and the CREATE operation.

And since I eliminated the update operations

they are not logged, although Debezium listens

the update events as well.

Remember that I update the outbox table

to keep the latest dates on that table

and do cross-checks in the business logic.

As I mentioned, it's also a good option

to insert and delete the outbox records immediately

and just use the transaction log

to get the outbox records through Debezium.

In that case, for deduplication of messages,

we would need another table,

such as a message log table that will have a unique key

with the ID of each message.

Remember, I do deduplication

using the data in the outbox table

since I do not delete them immediately,

and I can still use the cleaner scheduler

to clean the completed messages.

This will prevent the outbox table records getting bigger.

Finally, let's check the logs of the connector.

As you can see it registers the connectors

to listen to four outbox tables

and functioning correctly to get change data capture events

for each outbox table in our services.

Great, now we have completed the outbox pattern

with change data capture and Debezium.

As you see, we have the same functionality when compared

with the pulling the outbox table implementation.

All right, in the next lecture

I will create a load test using JMeter,

using three instances of each service,

order, payment and restaurant services,

and test the outbox pattern with change data capture.

Then I will compare it

with the pulling the outbox table approach.

I will also compare using pessimistic

and optimistic locking in customer credit checks,

as I mentioned, so I will see you in the next lecture.